import asyncio
import json
import os
from typing import Dict, List, Any, AsyncGenerator, Optional, Literal, TypedDict, Callable
from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain_mcp_adapters.client import MultiServerMCPClient, load_mcp_tools
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langgraph.graph import StateGraph, END, START
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.prebuilt import ToolNode
import yaml
import logging
from datetime import datetime
from enum import Enum


class WorkflowState(TypedDict):
    """ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì •ì˜"""
    messages: List[Any]
    user_query: str
    current_step: str
    tool_results: List[Dict[str, Any]]
    evaluation_results: List[Dict[str, Any]]
    confidence_score: float
    next_action: str
    iteration_count: int
    max_iterations: int
    final_answer: str
    reasoning_trace: List[str]
    # Human-in-the-loop ê´€ë ¨ í•„ë“œ
    human_approval_needed: bool
    human_input_requested: bool
    human_response: Optional[str]
    pending_decision: Optional[Dict[str, Any]]
    hitl_enabled: bool


class ToolEvaluationResult(Enum):
    """ë„êµ¬ í‰ê°€ ê²°ê³¼"""
    SUCCESS = "success"
    PARTIAL = "partial"
    FAILURE = "failure"
    NEEDS_MORE_INFO = "needs_more_info"


class HumanApprovalType(Enum):
    """Human approval íƒ€ì…"""
    TOOL_EXECUTION = "tool_execution"
    HIGH_IMPACT_DECISION = "high_impact_decision"
    LOW_CONFIDENCE = "low_confidence"
    FINAL_ANSWER = "final_answer"


class SupervisorService:
    """Human-in-the-loop ê¸°ëŠ¥ì„ í¬í•¨í•œ ë™ì  ì›Œí¬í”Œë¡œìš° ê¸°ë°˜ LangGraph MCP ì—ì´ì „íŠ¸ ì„œë¹„ìŠ¤"""

    def __init__(self):
        self.mcp_client = None
        self.model = None
        self.evaluator_model = None
        self.tools = []
        self.workflow = None
        self.checkpointer = InMemorySaver()
        self.timeout_seconds = 120

        # Human-in-the-loop ì„¤ì •
        self.hitl_config = {
            "enabled": True,
            "require_approval_for_tools": False,  # ë„êµ¬ ì‹¤í–‰ ì „ ìŠ¹ì¸ í•„ìš”
            "require_approval_for_low_confidence": True,  # ë‚®ì€ ì‹ ë¢°ë„ ì‹œ ìŠ¹ì¸ í•„ìš”
            "require_approval_for_final_answer": False,  # ìµœì¢… ë‹µë³€ ì „ ìŠ¹ì¸ í•„ìš”
            "confidence_threshold": 0.7,  # ì´ ê°’ ì´í•˜ë©´ human approval ìš”ì²­
            "high_impact_tools": ["file_operations", "external_api_calls", "system_commands"]  # ê³ ìœ„í—˜ ë„êµ¬
        }

        # Human input callback
        self.human_input_callback: Optional[Callable] = None

        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)

    def set_human_input_callback(self, callback: Callable[[str, Dict], str]):
        """Human input callback ì„¤ì •"""
        self.human_input_callback = callback

    def configure_hitl(self, **config):
        """Human-in-the-loop ì„¤ì • ì—…ë°ì´íŠ¸"""
        self.hitl_config.update(config)
        self.logger.info(f"HITL ì„¤ì • ì—…ë°ì´íŠ¸: {self.hitl_config}")

    async def initialize_agent(self,
                               model_name: str = "qwen2.5:32b",
                               evaluator_model_name: Optional[str] = None,
                               mcp_config: Optional[Dict] = None,
                               system_prompt: Optional[str] = None,
                               hitl_enabled: bool = True):
        """ë™ì  ì›Œí¬í”Œë¡œìš° ì—ì´ì „íŠ¸ ì´ˆê¸°í™”"""
        try:
            self.logger.info(f"HITL ì§€ì› ë™ì  ì›Œí¬í”Œë¡œìš° ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì‹œì‘: {model_name}")

            # HITL ì„¤ì •
            self.hitl_config["enabled"] = hitl_enabled

            # ê¸°ì¡´ í´ë¼ì´ì–¸íŠ¸ ì •ë¦¬
            await self.cleanup_mcp_client()

            # MCP ì„¤ì • ë¡œë“œ ë° ë„êµ¬ ì´ˆê¸°í™”
            if mcp_config is None:
                mcp_config = self.load_mcp_config()

            if mcp_config and mcp_config.get("mcpServers"):
                self.mcp_client = MultiServerMCPClient(mcp_config["mcpServers"])
                self.tools = await self.mcp_client.get_tools()
            else:
                self.tools = []

            # ë©”ì¸ ëª¨ë¸ê³¼ í‰ê°€ ëª¨ë¸ ì´ˆê¸°í™”
            self.model = self.create_model(model_name)
            self.evaluator_model = self.create_model(evaluator_model_name or model_name)

            # ë™ì  ì›Œí¬í”Œë¡œìš° ìƒì„±
            self.workflow = self._create_dynamic_workflow()

            self.logger.info(f"HITL ì§€ì› ë™ì  ì›Œí¬í”Œë¡œìš° ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ. ë„êµ¬ {len(self.tools)}ê°œ ë¡œë“œë¨")
            return True

        except Exception as e:
            self.logger.error(f"ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    def _create_dynamic_workflow(self) -> StateGraph:
        """Human-in-the-loop ê¸°ëŠ¥ì„ í¬í•¨í•œ ë™ì  ì›Œí¬í”Œë¡œìš° ìƒì„±"""
        workflow = StateGraph(WorkflowState)

        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("analyze_query", self._analyze_query)
        workflow.add_node("plan_execution", self._plan_execution)
        workflow.add_node("human_approval", self._human_approval)  # Human approval ë…¸ë“œ
        workflow.add_node("execute_tools", self._execute_tools)
        workflow.add_node("evaluate_results", self._evaluate_results)
        workflow.add_node("synthesize_answer", self._synthesize_answer)
        workflow.add_node("quality_check", self._quality_check)
        workflow.add_node("simple_answer", self._simple_answer)
        workflow.add_node("human_input", self._human_input)  # Human input ë…¸ë“œ

        # ì›Œí¬í”Œë¡œìš° ì—°ê²°
        workflow.add_edge(START, "analyze_query")

        # ë¶„ì„ ë‹¨ê³„ í›„ ì¡°ê±´ë¶€ ë¶„ê¸°
        workflow.add_conditional_edges(
            "analyze_query",
            self._decide_after_analysis,
            {
                "simple": "simple_answer",
                "complex": "plan_execution"
            }
        )

        workflow.add_edge("simple_answer", END)

        # ê³„íš í›„ Human approval ì²´í¬
        workflow.add_conditional_edges(
            "plan_execution",
            self._decide_after_planning,
            {
                "execute": "execute_tools",
                "need_approval": "human_approval",  # Human approval í•„ìš”
                "skip_to_synthesize": "synthesize_answer",
                "end": END
            }
        )

        # Human approval í›„ ë¶„ê¸°
        workflow.add_conditional_edges(
            "human_approval",
            self._decide_after_approval,
            {
                "approved": "execute_tools",
                "rejected": "plan_execution",  # ë‹¤ì‹œ ê³„íš
                "modified": "plan_execution",  # ìˆ˜ì •ëœ ê³„íšìœ¼ë¡œ
                "need_input": "human_input"  # ì¶”ê°€ ì…ë ¥ í•„ìš”
            }
        )

        # Human input í›„ ê³„íšìœ¼ë¡œ ëŒì•„ê°€ê¸°
        workflow.add_edge("human_input", "plan_execution")

        workflow.add_edge("execute_tools", "evaluate_results")

        # í‰ê°€ í›„ ë‚®ì€ ì‹ ë¢°ë„ ì‹œ Human approval
        workflow.add_conditional_edges(
            "evaluate_results",
            self._decide_next_step,
            {
                "continue": "plan_execution",
                "synthesize": "synthesize_answer",
                "need_approval": "human_approval",  # ë‚®ì€ ì‹ ë¢°ë„ë¡œ ì¸í•œ approval
                "end": END
            }
        )

        workflow.add_edge("synthesize_answer", "quality_check")
        workflow.add_conditional_edges(
            "quality_check",
            self._decide_final_step,
            {
                "approved": END,
                "retry": "plan_execution",
                "need_approval": "human_approval"  # ìµœì¢… ë‹µë³€ ìŠ¹ì¸
            }
        )

        return workflow.compile(checkpointer=self.checkpointer)

    async def _analyze_query(self, state: WorkflowState) -> WorkflowState:
        """ì‚¬ìš©ì ì¿¼ë¦¬ ë¶„ì„"""
        self.logger.info("ì‚¬ìš©ì ì¿¼ë¦¬ ë¶„ì„ ì¤‘...")

        analysis_prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ê´€ì ì—ì„œ ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ì„¸ìš”:
1. ì¿¼ë¦¬ì˜ ì˜ë„ì™€ ëª©ì 
2. í•„ìš”í•œ ì •ë³´ì˜ ì¢…ë¥˜
3. ì í•©í•œ ë„êµ¬ë“¤
4. ì˜ˆìƒë˜ëŠ” ë³µì¡ë„ (1-10)
5. ë‹¨ê³„ë³„ í•´ê²° ê³„íš

**ì¤‘ìš”**: ë‹¤ìŒê³¼ ê°™ì€ ê²½ìš° "ë„êµ¬ ë¶ˆí•„ìš”"ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”:
- ë‹¨ìˆœí•œ ì¸ì‚¬ (ì•ˆë…•, ì•ˆë…•í•˜ì„¸ìš”, ë°˜ê°€ì›Œìš” ë“±)
- ì¼ë°˜ì ì¸ ì§ˆë¬¸ì´ë‚˜ ìƒì‹ ë¬¸ì˜
- ì°½ì˜ì  ê¸€ì“°ê¸° ìš”ì²­
- ì„¤ëª…ì´ë‚˜ ì •ì˜ ìš”ì²­
- ë„êµ¬ ì—†ì´ ëª¨ë¸ ì§€ì‹ìœ¼ë¡œ ì¶©ë¶„íˆ ë‹µë³€ ê°€ëŠ¥í•œ ê²½ìš°

Available tools: {tools}

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
  "analysis": {{
    "intention_and_purpose": "ì¿¼ë¦¬ì˜ ì˜ë„ì™€ ëª©ì ",
    "type_of_information_required": "í•„ìš”í•œ ì •ë³´ ì¢…ë¥˜",
    "suitable_tools": "ì í•©í•œ ë„êµ¬ë“¤ ë˜ëŠ” 'ë„êµ¬ ë¶ˆí•„ìš”'",
    "expected_complexity": 1-10,
    "step_by_step_solution_plan": ["í•´ê²° ê³„íš"],
    "requires_tools": true/false
  }}
}}"""),
            ("human", "ì¿¼ë¦¬: {query}")
        ])

        tool_names = [tool.name if hasattr(tool, 'name') else str(tool) for tool in self.tools]

        try:
            response = await self.model.ainvoke(
                analysis_prompt.format_messages(
                    query=state["user_query"],
                    tools=", ".join(tool_names) if tool_names else "ì—†ìŒ"
                )
            )

            # JSON íŒŒì‹± ì‹œë„
            try:
                analysis_result = json.loads(response.content)
                requires_tools = analysis_result.get("analysis", {}).get("requires_tools", True)
                suitable_tools = analysis_result.get("analysis", {}).get("suitable_tools", "")

                # ë„êµ¬ê°€ í•„ìš” ì—†ëŠ” ê²½ìš° ì²´í¬
                if (not requires_tools or
                        "ë„êµ¬ ë¶ˆí•„ìš”" in suitable_tools or
                        "íŠ¹ì • ë„êµ¬ê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤" in suitable_tools or
                        "ë‹¨ìˆœí•œ í…ìŠ¤íŠ¸ ì‘ë‹µìœ¼ë¡œ ì¶©ë¶„í•©ë‹ˆë‹¤" in suitable_tools):
                    self.logger.info("ë„êµ¬ê°€ í•„ìš” ì—†ëŠ” ì¿¼ë¦¬ë¡œ íŒë‹¨ - ì§ì ‘ ë‹µë³€ ìƒì„±")
                    state["current_step"] = "simple_query"
                    state["reasoning_trace"].append("ë„êµ¬ê°€ í•„ìš” ì—†ëŠ” ê°„ë‹¨í•œ ì¿¼ë¦¬ë¡œ íŒë‹¨ë˜ì–´ ì§ì ‘ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.")
                    return state

            except json.JSONDecodeError:
                self.logger.warning("ë¶„ì„ ê²°ê³¼ JSON íŒŒì‹± ì‹¤íŒ¨ - ê¸°ë³¸ ì›Œí¬í”Œë¡œìš° ì§„í–‰")

            # ë¶„ì„ ê²°ê³¼ë¥¼ ìƒíƒœì— ì €ì¥
            state["reasoning_trace"].append(f"ì¿¼ë¦¬ ë¶„ì„: {response.content}")
            state["current_step"] = "query_analyzed"

        except Exception as e:
            self.logger.error(f"ì¿¼ë¦¬ ë¶„ì„ ì‹¤íŒ¨: {e}")
            state["reasoning_trace"].append(f"ì¿¼ë¦¬ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            state["current_step"] = "query_analyzed"  # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰

        return state

    async def _plan_execution(self, state: WorkflowState) -> WorkflowState:
        """ì‹¤í–‰ ê³„íš ìˆ˜ë¦½"""
        self.logger.info("ì‹¤í–‰ ê³„íš ìˆ˜ë¦½ ì¤‘...")

        # ì²« ë²ˆì§¸ ë°˜ë³µì´ë©´ ë¬´ì¡°ê±´ ë„êµ¬ ì‹¤í–‰ ê³„íšì„ ì„¸ì›Œì•¼ í•¨
        if state["iteration_count"] == 0:
            self.logger.info("ì²« ë²ˆì§¸ ë°˜ë³µ - ë„êµ¬ ì‹¤í–‰ ê³„íš ìˆ˜ë¦½")
        else:
            # ì´ì „ í‰ê°€ ê²°ê³¼ í™•ì¸ - ì´ë¯¸ ì¶©ë¶„í•œ ê²°ê³¼ê°€ ìˆëŠ”ì§€ ì²´í¬
            if state["evaluation_results"]:
                latest_evaluation = state["evaluation_results"][-1]
                confidence = latest_evaluation.get("confidence", 0.0)
                evaluation_type = latest_evaluation.get("evaluation", "")

                # ì´ë¯¸ ë†’ì€ ì‹ ë¢°ë„ë¥¼ ë‹¬ì„±í–ˆë‹¤ë©´ ì¶”ê°€ ë„êµ¬ ì‹¤í–‰ ìƒëµ
                if confidence >= 0.95 or evaluation_type == ToolEvaluationResult.SUCCESS.value:
                    self.logger.info(f"ì´ë¯¸ ì¶©ë¶„í•œ ê²°ê³¼ ë‹¬ì„± (ì‹ ë¢°ë„: {confidence:.2f}) - ë„êµ¬ ì‹¤í–‰ ìƒëµ")
                    state["reasoning_trace"].append("ì´ë¯¸ ì¶©ë¶„í•œ ê²°ê³¼ë¥¼ ì–»ì—ˆìœ¼ë¯€ë¡œ ì¶”ê°€ ë„êµ¬ ì‹¤í–‰ì„ ìƒëµí•©ë‹ˆë‹¤.")
                    state["current_step"] = "plan_skipped"
                    return state

        # ì´ì „ ë„êµ¬ ê²°ê³¼ ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€
        executed_tools = set()
        for tool_result in state["tool_results"]:
            if tool_result.get("success", False):
                executed_tools.add(tool_result.get("tool_name", ""))

        planning_prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ê³  ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ì„ í™•ì¸í•˜ì—¬ ì‹¤í–‰ ê³„íšì„ ì„¸ìš°ì„¸ìš”.

**ì¤‘ìš” ê·œì¹™**:
1. ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ì—ì„œë§Œ ë„êµ¬ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2. ì‚¬ìš©ì ìš”ì²­ì— ì í•©í•œ ë„êµ¬ê°€ ì—†ë‹¤ë©´ "ì í•©í•œ ë„êµ¬ ì—†ìŒ"ì´ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”.
3. ì´ë¯¸ ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰ëœ ë„êµ¬ëŠ” ë‹¤ì‹œ ì‹¤í–‰í•˜ì§€ ë§ˆì„¸ìš”.
4. ì²« ë²ˆì§¸ ì‹¤í–‰ì´ë¼ë©´ ì‚¬ìš©ì ì¿¼ë¦¬ì— ê°€ì¥ ì í•©í•œ ë„êµ¬ë¥¼ ì„ íƒí•˜ì„¸ìš”.

í˜„ì¬ ìƒí™©:
- ì‚¬ìš©ì ì¿¼ë¦¬: {query}
- í˜„ì¬ ë°˜ë³µ: {iteration}/{max_iterations}
- ì´ì „ ë„êµ¬ ê²°ê³¼: {tool_results}
- ì´ì „ í‰ê°€ ê²°ê³¼: {evaluation_results}
- ì´ë¯¸ ì‹¤í–‰ëœ ë„êµ¬: {executed_tools}

**ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬**: {tools}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ê³„íšì„ ì œì‹œí•˜ì„¸ìš”:
- ë‹¤ìŒì— ì‚¬ìš©í•  ë„êµ¬ë“¤: [ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ë„êµ¬ë§Œ ë‚˜ì—´]
- ê° ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ : [êµ¬ì²´ì  ì´ìœ ]
- ì˜ˆìƒë˜ëŠ” ê²°ê³¼: [ì˜ˆìƒ ê²°ê³¼]
- ì í•©í•œ ë„êµ¬ê°€ ì—†ë‹¤ë©´: "ì í•©í•œ ë„êµ¬ ì—†ìŒ - ì‚¬ìš©ìì—ê²Œ ì§ì ‘ ì„¤ëª… í•„ìš”"
- ì¶”ê°€ ë„êµ¬ê°€ í•„ìš”í•˜ì§€ ì•Šë‹¤ë©´: "ì¶”ê°€ ë„êµ¬ ë¶ˆí•„ìš”"

**ì£¼ì˜**: ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë„êµ¬(ì˜ˆ: get_weather, search_web ë“±)ë¥¼ ì œì•ˆí•˜ì§€ ë§ˆì„¸ìš”."""),
            ("human", "ë‹¤ìŒ ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”.")
        ])

        tool_names = [tool.name if hasattr(tool, 'name') else str(tool) for tool in self.tools]

        try:
            response = await self.model.ainvoke(
                planning_prompt.format_messages(
                    query=state["user_query"],
                    iteration=state["iteration_count"],
                    max_iterations=state["max_iterations"],
                    tool_results=state["tool_results"][-3:] if state["tool_results"] else "ì—†ìŒ",
                    evaluation_results=state["evaluation_results"][-3:] if state["evaluation_results"] else "ì—†ìŒ",
                    executed_tools=", ".join(executed_tools) if executed_tools else "ì—†ìŒ",
                    tools=", ".join(tool_names) if tool_names else "ì—†ìŒ"
                )
            )

            response_content = response.content.lower()

            # ì í•©í•œ ë„êµ¬ê°€ ì—†ëŠ” ê²½ìš° ì²´í¬
            if any(phrase in response_content for phrase in ["ì í•©í•œ ë„êµ¬ ì—†ìŒ", "ì‚¬ìš©ìì—ê²Œ ì§ì ‘ ì„¤ëª…", "ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´"]):
                self.logger.info("ì‚¬ìš©ì ìš”ì²­ì— ì í•©í•œ ë„êµ¬ê°€ ì—†ìŒ - ë„êµ¬ ì‹¤í–‰ ì—†ì´ ì§ì ‘ ë‹µë³€")
                state["reasoning_trace"].append("ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ì¤‘ ì‚¬ìš©ì ìš”ì²­ì— ì í•©í•œ ë„êµ¬ê°€ ì—†ì–´ ì§ì ‘ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.")
                state["current_step"] = "no_suitable_tools"
                return state

            # ì²« ë²ˆì§¸ ë°˜ë³µì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ "ì¶”ê°€ ë„êµ¬ ë¶ˆí•„ìš”" ì²´í¬
            if state["iteration_count"] > 0:
                if any(phrase in response_content for phrase in ["ì¶”ê°€ ë„êµ¬ ë¶ˆí•„ìš”", "ì¶”ê°€ì ì¸ ë„êµ¬", "ë” ì´ìƒ", "í•„ìš”í•˜ì§€ ì•Š"]):
                    self.logger.info("ê³„íš ë‹¨ê³„ì—ì„œ ì¶”ê°€ ë„êµ¬ ì‹¤í–‰ì´ ë¶ˆí•„ìš”í•˜ë‹¤ê³  íŒë‹¨ë¨")
                    state["reasoning_trace"].append("ì¶”ê°€ ë„êµ¬ ì‹¤í–‰ì´ ë¶ˆí•„ìš”í•˜ë‹¤ê³  íŒë‹¨ë˜ì–´ ê³„íšì„ ìƒëµí•©ë‹ˆë‹¤.")
                    state["current_step"] = "plan_skipped"
                    return state

            state["reasoning_trace"].append(f"ì‹¤í–‰ ê³„íš: {response.content}")
            state["current_step"] = "plan_ready"

        except Exception as e:
            self.logger.error(f"ì‹¤í–‰ ê³„íš ìˆ˜ë¦½ ì‹¤íŒ¨: {e}")
            state["reasoning_trace"].append(f"ì‹¤í–‰ ê³„íš ìˆ˜ë¦½ ì‹¤íŒ¨: {str(e)}")
            state["current_step"] = "plan_failed"

        return state

    async def _human_approval(self, state: WorkflowState) -> WorkflowState:
        """Human approval ìš”ì²­ ì²˜ë¦¬"""
        self.logger.info("Human approval ìš”ì²­ ì¤‘...")

        if not self.hitl_config.get("enabled", False):
            self.logger.info("HITLì´ ë¹„í™œì„±í™”ë¨ - ìë™ ìŠ¹ì¸")
            state["human_response"] = "approved"
            return state

        pending_decision = state.get("pending_decision", {})
        approval_type = pending_decision.get("type", "unknown")

        # ìŠ¹ì¸ ìš”ì²­ ë©”ì‹œì§€ êµ¬ì„±
        approval_message = self._create_approval_message(state, approval_type, pending_decision)

        try:
            if self.human_input_callback:
                # Callbackì„ í†µí•´ human input ìš”ì²­
                human_response = self.human_input_callback(approval_message, pending_decision)
                state["human_response"] = human_response
                state["reasoning_trace"].append(f"Human approval ì‘ë‹µ: {human_response}")
            else:
                # Callbackì´ ì—†ìœ¼ë©´ ìë™ ìŠ¹ì¸ (ê°œë°œ/í…ŒìŠ¤íŠ¸ ìš©)
                self.logger.warning("Human input callbackì´ ì„¤ì •ë˜ì§€ ì•ŠìŒ - ìë™ ìŠ¹ì¸")
                state["human_response"] = "approved"

        except Exception as e:
            self.logger.error(f"Human approval ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            state["human_response"] = "approved"  # ì‹¤íŒ¨ ì‹œ ìë™ ìŠ¹ì¸

        state["human_approval_needed"] = False
        return state

    async def _human_input(self, state: WorkflowState) -> WorkflowState:
        """Human input ìš”ì²­ ì²˜ë¦¬"""
        self.logger.info("Human input ìš”ì²­ ì¤‘...")

        if not self.hitl_config.get("enabled", False):
            state["human_response"] = "continue"
            return state

        input_message = "ì¶”ê°€ ì •ë³´ë‚˜ ì§€ì‹œì‚¬í•­ì„ ì œê³µí•´ì£¼ì„¸ìš”:"

        try:
            if self.human_input_callback:
                human_input = self.human_input_callback(input_message, {"type": "input_request"})
                state["human_response"] = human_input
                state["reasoning_trace"].append(f"Human input ìˆ˜ì‹ : {human_input}")
                # ë°›ì€ ì…ë ¥ì„ ì‚¬ìš©ì ì¿¼ë¦¬ì— ì¶”ê°€
                state["user_query"] += f"\n[ì¶”ê°€ ì •ë³´: {human_input}]"
            else:
                self.logger.warning("Human input callbackì´ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
                state["human_response"] = "continue"

        except Exception as e:
            self.logger.error(f"Human input ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            state["human_response"] = "continue"

        state["human_input_requested"] = False
        return state

    async def _execute_tools(self, state: WorkflowState) -> WorkflowState:
        """ì‹¤ì œ MCP ë„êµ¬ ì‹¤í–‰"""
        self.logger.info("MCP ë„êµ¬ ì‹¤í–‰ ë‹¨ê³„...")

        # ê³„íšì´ ìƒëµë˜ì—ˆê±°ë‚˜ ì¶”ê°€ ë„êµ¬ê°€ ë¶ˆí•„ìš”í•œ ê²½ìš° ê±´ë„ˆë›°ê¸°
        if state["current_step"] in ["plan_skipped", "plan_failed"]:
            self.logger.info("ê³„íš ë‹¨ê³„ì—ì„œ ë„êµ¬ ì‹¤í–‰ì´ ë¶ˆí•„ìš”í•˜ë‹¤ê³  íŒë‹¨ë¨ - ë„êµ¬ ì‹¤í–‰ ìƒëµ")
            state["current_step"] = "tools_skipped"
            return state

        # ì´ë¯¸ ì¶©ë¶„í•œ ê²°ê³¼ê°€ ìˆëŠ”ì§€ ì¬í™•ì¸
        if state["evaluation_results"]:
            latest_evaluation = state["evaluation_results"][-1]
            confidence = latest_evaluation.get("confidence", 0.0)
            if confidence >= 1.0:
                self.logger.info(f"ì™„ë²½í•œ ì‹ ë¢°ë„({confidence:.2f}) ë‹¬ì„± - ë„êµ¬ ì‹¤í–‰ ìƒëµ")
                state["current_step"] = "tools_skipped"
                return state

        try:
            if not self.tools:
                self.logger.info("ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ê°€ ì—†ìŒ. ëª¨ë¸ ì§€ì‹ë§Œìœ¼ë¡œ ë‹µë³€ ìƒì„±")
                state["current_step"] = "tools_executed"
                return state

            # ì´ë¯¸ ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰ëœ ë„êµ¬ë“¤ í™•ì¸
            executed_tools = set()
            for tool_result in state["tool_results"]:
                if tool_result.get("success", False):
                    executed_tools.add(tool_result.get("tool_name", ""))

            # ì•„ì§ ì‹¤í–‰ë˜ì§€ ì•Šì€ ë„êµ¬ê°€ ìˆëŠ”ì§€ í™•ì¸
            available_tools = [tool for tool in self.tools
                               if getattr(tool, 'name', str(tool)) not in executed_tools]

            if not available_tools:
                self.logger.info("ëª¨ë“  ê´€ë ¨ ë„êµ¬ê°€ ì´ë¯¸ ì‹¤í–‰ë¨ - ì¶”ê°€ ì‹¤í–‰ ìƒëµ")
                state["current_step"] = "tools_skipped"
                return state

            # ì²« ë²ˆì§¸ ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ì‹¤í–‰
            selected_tool = available_tools[0]
            tool_name = getattr(selected_tool, 'name', 'mcp_tool')

            self.logger.info(f"ìƒˆë¡œìš´ ë„êµ¬ '{tool_name}' ì‹¤í–‰ ì¤‘...")

            # ì‹¤í–‰ ê³„íšì—ì„œ ì¶”ì¶œí•œ ë„êµ¬ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¤ì œ ë„êµ¬ ì‹¤í–‰
            tool_node = ToolNode([selected_tool])

            # ë„êµ¬ ì‹¤í–‰ì„ ìœ„í•œ ë©”ì‹œì§€ êµ¬ì„±
            tool_message = AIMessage(
                content="",
                tool_calls=[{
                    "name": tool_name,
                    "args": {"query": state["user_query"]},
                    "id": f"call_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                }]
            )

            # ì‹¤ì œ ë„êµ¬ ì‹¤í–‰
            tool_response = await tool_node.ainvoke({"messages": [tool_message]})

            if tool_response and "messages" in tool_response:
                for msg in tool_response["messages"]:
                    if hasattr(msg, 'content'):
                        tool_result = {
                            "tool_name": tool_name,
                            "input": state["user_query"],
                            "output": msg.content,
                            "timestamp": datetime.now().isoformat(),
                            "success": True
                        }

                        state["tool_results"].append(tool_result)
                        state["reasoning_trace"].append(f"ìƒˆë¡œìš´ MCP ë„êµ¬ ì‹¤í–‰ ì™„ë£Œ: {tool_result['tool_name']}")
                        self.logger.info(f"ë„êµ¬ '{tool_result['tool_name']}' ì‹¤í–‰ ì„±ê³µ")
            else:
                self.logger.warning("ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ")

            state["current_step"] = "tools_executed"

        except Exception as e:
            self.logger.error(f"MCP ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            # ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨ ì‹œì—ë„ ëª¨ë¸ ì§€ì‹ìœ¼ë¡œ ë‹µë³€ ì‹œë„
            tool_result = {
                "tool_name": "fallback_knowledge",
                "input": state["user_query"],
                "output": f"ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨ë¡œ ëª¨ë¸ ì§€ì‹ ê¸°ë°˜ ë‹µë³€ ì‹œë„: {str(e)}",
                "timestamp": datetime.now().isoformat(),
                "success": False
            }
            state["tool_results"].append(tool_result)
            state["current_step"] = "tools_executed"

        return state

    async def _evaluate_results(self, state: WorkflowState) -> WorkflowState:
        """ë„êµ¬ ê²°ê³¼ í‰ê°€"""
        self.logger.info("ë„êµ¬ ê²°ê³¼ í‰ê°€ ì¤‘...")

        if not state["tool_results"]:
            state["evaluation_results"].append({
                "evaluation": ToolEvaluationResult.FAILURE.value,
                "confidence": 0.0,
                "reason": "ì‹¤í–‰ëœ ë„êµ¬ê°€ ì—†ìŒ"
            })
            return state

        latest_result = state["tool_results"][-1]

        evaluation_prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ê²°ê³¼ë¥¼ í‰ê°€í•˜ì„¸ìš”:

1. ì •í™•ì„±: ê²°ê³¼ê°€ ì¿¼ë¦¬ì— ì •í™•íˆ ë‹µí•˜ëŠ”ê°€?
2. ì™„ì„±ë„: ë‹µë³€ì´ ì™„ì „í•œê°€, ì•„ë‹ˆë©´ ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•œê°€?
3. ì‹ ë¢°ì„±: ê²°ê³¼ë¥¼ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ”ê°€?
4. ê´€ë ¨ì„±: ì‚¬ìš©ì ì¿¼ë¦¬ì™€ ê´€ë ¨ì´ ìˆëŠ”ê°€?

í‰ê°€ ê²°ê³¼ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ:
{{
  "evaluation": "success|partial|failure|needs_more_info",
  "confidence": 0.0-1.0,
  "reason": "í‰ê°€ ì´ìœ ",
  "missing_info": ["ë¶€ì¡±í•œ ì •ë³´ ëª©ë¡"],
  "next_steps": ["ì œì•ˆí•˜ëŠ” ë‹¤ìŒ ë‹¨ê³„"]
}}"""),
            ("human", """
ì‚¬ìš©ì ì¿¼ë¦¬: {query}
ë„êµ¬ ê²°ê³¼: {tool_result}
ì´ì „ ì»¨í…ìŠ¤íŠ¸: {context}

í‰ê°€í•´ì£¼ì„¸ìš”.""")
        ])

        try:
            response = await self.evaluator_model.ainvoke(
                evaluation_prompt.format_messages(
                    query=state["user_query"],
                    tool_result=latest_result,
                    context=state["reasoning_trace"][-3:]
                )
            )

            # JSON íŒŒì‹± ì‹œë„
            try:
                evaluation = json.loads(response.content)
            except json.JSONDecodeError:
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í‰ê°€
                evaluation = {
                    "evaluation": ToolEvaluationResult.PARTIAL.value,
                    "confidence": 0.5,
                    "reason": "í‰ê°€ ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨",
                    "missing_info": [],
                    "next_steps": []
                }

            state["evaluation_results"].append(evaluation)
            state["confidence_score"] = evaluation.get("confidence", 0.5)
            state["reasoning_trace"].append(f"í‰ê°€ ê²°ê³¼: {evaluation['evaluation']} (ì‹ ë¢°ë„: {evaluation['confidence']})")

        except Exception as e:
            self.logger.error(f"ê²°ê³¼ í‰ê°€ ì‹¤íŒ¨: {e}")
            state["evaluation_results"].append({
                "evaluation": ToolEvaluationResult.FAILURE.value,
                "confidence": 0.0,
                "reason": f"í‰ê°€ ì‹¤íŒ¨: {str(e)}"
            })

        state["current_step"] = "results_evaluated"
        state["iteration_count"] += 1

        return state

    async def _synthesize_answer(self, state: WorkflowState) -> WorkflowState:
        """ìµœì¢… ë‹µë³€ í•©ì„±"""
        self.logger.info("ìµœì¢… ë‹µë³€ í•©ì„± ì¤‘...")

        # ì í•©í•œ ë„êµ¬ê°€ ì—†ëŠ” ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
        if state.get("current_step") == "no_suitable_tools":
            synthesis_prompt = ChatPromptTemplate.from_messages([
                ("system", """ë‹¹ì‹ ì€ ì‚¬ìš©ìì—ê²Œ ì •ì¤‘í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
í˜„ì¬ ìƒí™©ì—ì„œëŠ” ì‚¬ìš©ìì˜ ìš”ì²­ì— ì í•©í•œ ë„êµ¬ê°€ ì—†ì–´ì„œ ì‹¤ì‹œê°„ ì •ë³´ë¥¼ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

ë‹¤ìŒê³¼ ê°™ì´ ë‹µë³€í•˜ì„¸ìš”:
1. ì‚¬ìš©ìì˜ ìš”ì²­ì„ ì´í•´í–ˆìŒì„ í‘œí˜„
2. í˜„ì¬ í•´ë‹¹ ì •ë³´ë¥¼ ì œê³µí•  ìˆ˜ ìˆëŠ” ë„êµ¬ê°€ ì—†ìŒì„ ì •ì¤‘í•˜ê²Œ ì„¤ëª…
3. ì¼ë°˜ì ì¸ ì •ë³´ë‚˜ ëŒ€ì•ˆì„ ì œê³µ (ê°€ëŠ¥í•œ ê²½ìš°)
4. ë‹¤ë¥¸ ë°©ë²•ì´ë‚˜ ì¶”ì²œ ì‚¬í•­ ì œì‹œ

ì‚¬ìš©ì ìš”ì²­: {query}
ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë“¤: {available_tools}
ì¶”ë¡  ê³¼ì •: {reasoning_trace}"""),
                ("human", "ì ì ˆí•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.")
            ])

            tool_names = [tool.name if hasattr(tool, 'name') else str(tool) for tool in self.tools]

            try:
                response = await self.model.ainvoke(
                    synthesis_prompt.format_messages(
                        query=state["user_query"],
                        available_tools=", ".join(tool_names) if tool_names else "ì—†ìŒ",
                        reasoning_trace=state["reasoning_trace"]
                    )
                )

                state["final_answer"] = response.content
                state["current_step"] = "answer_synthesized"

            except Exception as e:
                self.logger.error(f"ë‹µë³€ í•©ì„± ì‹¤íŒ¨: {e}")
                state[
                    "final_answer"] = f"ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ '{state['user_query']}'ì— ëŒ€í•œ ì‹¤ì‹œê°„ ì •ë³´ë¥¼ ì œê³µí•  ìˆ˜ ìˆëŠ” ë„êµ¬ê°€ ì—†ì–´ì„œ ì •í™•í•œ ë‹µë³€ì„ ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”."

        else:
            # ê¸°ì¡´ ë¡œì§: ë„êµ¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ í•©ì„±
            synthesis_prompt = ChatPromptTemplate.from_messages([
                ("system", """ë‹¹ì‹ ì€ ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ì„ í•©ì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì •ë³´ë“¤ì„ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ì ì¿¼ë¦¬ì— ëŒ€í•œ ì •í™•í•˜ê³  ì™„ì „í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”:

- ì‚¬ìš©ì ì¿¼ë¦¬: {query}
- ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë“¤: {tool_results}
- í‰ê°€ ê²°ê³¼ë“¤: {evaluation_results}
- ì¶”ë¡  ê³¼ì •: {reasoning_trace}

ë‹µë³€ì€ ë‹¤ìŒê³¼ ê°™ì´ êµ¬ì„±í•˜ì„¸ìš”:
1. ì§ì ‘ì ì¸ ë‹µë³€
2. ê·¼ê±°ê°€ ë˜ëŠ” ì •ë³´
3. ì‹ ë¢°ë„ ë° í•œê³„ì  (í•„ìš”ì‹œ)"""),
                ("human", "ìµœì¢… ë‹µë³€ì„ í•©ì„±í•´ì£¼ì„¸ìš”.")
            ])

            try:
                response = await self.model.ainvoke(
                    synthesis_prompt.format_messages(
                        query=state["user_query"],
                        tool_results=state["tool_results"],
                        evaluation_results=state["evaluation_results"],
                        reasoning_trace=state["reasoning_trace"]
                    )
                )

                state["final_answer"] = response.content
                state["current_step"] = "answer_synthesized"

            except Exception as e:
                self.logger.error(f"ë‹µë³€ í•©ì„± ì‹¤íŒ¨: {e}")
                state["final_answer"] = f"ë‹µë³€ í•©ì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

        return state

    async def _quality_check(self, state: WorkflowState) -> WorkflowState:
        """í’ˆì§ˆ ê²€ì‚¬"""
        self.logger.info("ë‹µë³€ í’ˆì§ˆ ê²€ì‚¬ ì¤‘...")

        quality_prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ë‹µë³€ í’ˆì§ˆì„ ê²€ì‚¬í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ë‹µë³€ì„ í‰ê°€í•˜ì„¸ìš”:

1. ì™„ì„±ë„: ì¿¼ë¦¬ì— ì™„ì „íˆ ë‹µí–ˆëŠ”ê°€?
2. ì •í™•ì„±: ì œê³µëœ ì •ë³´ê°€ ì •í™•í•œê°€?
3. ëª…í™•ì„±: ì´í•´í•˜ê¸° ì‰¬ìš´ê°€?
4. ê´€ë ¨ì„±: ì‚¬ìš©ì ìš”ì²­ê³¼ ê´€ë ¨ì´ ìˆëŠ”ê°€?

'approved' ë˜ëŠ” 'retry' ì¤‘ í•˜ë‚˜ë¡œ ì‘ë‹µí•˜ì„¸ìš”."""),
            ("human", """
ì‚¬ìš©ì ì¿¼ë¦¬: {query}
ìƒì„±ëœ ë‹µë³€: {answer}
í’ˆì§ˆì„ í‰ê°€í•´ì£¼ì„¸ìš”.""")
        ])

        try:
            response = await self.evaluator_model.ainvoke(
                quality_prompt.format_messages(
                    query=state["user_query"],
                    answer=state["final_answer"]
                )
            )

            state["next_action"] = "approved" if "approved" in response.content.lower() else "retry"
            state["reasoning_trace"].append(f"í’ˆì§ˆ ê²€ì‚¬: {state['next_action']}")

        except Exception as e:
            self.logger.error(f"í’ˆì§ˆ ê²€ì‚¬ ì‹¤íŒ¨: {e}")
            state["next_action"] = "approved"  # ì‹¤íŒ¨ ì‹œ ìŠ¹ì¸ìœ¼ë¡œ ì²˜ë¦¬

        return state

    async def _simple_answer(self, state: WorkflowState) -> WorkflowState:
        """ë„êµ¬ ì—†ì´ ê°„ë‹¨í•œ ì§ì ‘ ë‹µë³€"""
        self.logger.info("ê°„ë‹¨í•œ ì§ì ‘ ë‹µë³€ ìƒì„± ì¤‘...")

        simple_prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ê°„ë‹¨í•œ ì¿¼ë¦¬ì— ëŒ€í•´ ìì—°ìŠ¤ëŸ½ê³  ì ì ˆí•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

íŠ¹ë³„í•œ ë„êµ¬ë‚˜ ì‹¤ì‹œê°„ ì •ë³´ê°€ í•„ìš”í•˜ì§€ ì•Šì€ ê°„ë‹¨í•œ ì§ˆë¬¸ì´ë¯€ë¡œ, 
ë‹¹ì‹ ì˜ ê¸°ë³¸ ì§€ì‹ê³¼ ëŒ€í™” ëŠ¥ë ¥ì„ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.

ë‹µë³€ì€ ë‹¤ìŒê³¼ ê°™ì´ ì‘ì„±í•˜ì„¸ìš”:
- ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•œ í†¤
- ì ì ˆí•œ ê¸¸ì´ (ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ)
- í•„ìš”ì‹œ ì¶”ê°€ ë„ì›€ ì œì•ˆ"""),
            ("human", "ì‚¬ìš©ì ì¿¼ë¦¬: {query}")
        ])

        try:
            response = await self.model.ainvoke(
                simple_prompt.format_messages(query=state["user_query"])
            )

            state["final_answer"] = response.content
            state["current_step"] = "simple_answer_generated"
            state["reasoning_trace"].append("ê°„ë‹¨í•œ ì§ì ‘ ë‹µë³€ ìƒì„± ì™„ë£Œ")

        except Exception as e:
            self.logger.error(f"ê°„ë‹¨í•œ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°± ë‹µë³€
            if any(greeting in state["user_query"].lower() for greeting in ["ì•ˆë…•", "ë°˜ê°€", "hi", "hello"]):
                state["final_answer"] = "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
            else:
                state["final_answer"] = "ë„¤, ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
            state["current_step"] = "simple_answer_generated"

        return state

    def _create_approval_message(self, state: WorkflowState, approval_type: str, pending_decision: Dict) -> str:
        """ìŠ¹ì¸ ìš”ì²­ ë©”ì‹œì§€ ìƒì„±"""
        if approval_type == HumanApprovalType.TOOL_EXECUTION.value:
            tool_name = pending_decision.get("tool_name", "unknown")
            tool_args = pending_decision.get("tool_args", {})
            return f"""
ğŸ¤– ë„êµ¬ ì‹¤í–‰ ìŠ¹ì¸ ìš”ì²­

ë„êµ¬ëª…: {tool_name}
ì¸ìˆ˜: {json.dumps(tool_args, ensure_ascii=False, indent=2)}
ì´ìœ : {pending_decision.get('reason', 'ì‚¬ìš©ì ìš”ì²­ ì²˜ë¦¬')}

ìŠ¹ì¸í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (approved/rejected/modified)
"""

        elif approval_type == HumanApprovalType.LOW_CONFIDENCE.value:
            confidence = pending_decision.get("confidence", 0.0)
            return f"""
âš ï¸ ë‚®ì€ ì‹ ë¢°ë„ ê²°ê³¼ ìŠ¹ì¸ ìš”ì²­

í˜„ì¬ ì‹ ë¢°ë„: {confidence:.2f}
ê²°ê³¼: {pending_decision.get('result', '')}
ì´ìœ : ì‹ ë¢°ë„ê°€ ì„ê³„ê°’({self.hitl_config['confidence_threshold']}) ì´í•˜ì…ë‹ˆë‹¤.

ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (approved/rejected/need_input)
"""

        elif approval_type == HumanApprovalType.FINAL_ANSWER.value:
            answer = pending_decision.get("answer", "")
            return f"""
âœ… ìµœì¢… ë‹µë³€ ìŠ¹ì¸ ìš”ì²­

ë‹µë³€:
{answer}

ì´ ë‹µë³€ì„ ì‚¬ìš©ìì—ê²Œ ì œê³µí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (approved/rejected/modified)
"""

        else:
            return f"""
â“ ìŠ¹ì¸ ìš”ì²­

ë‚´ìš©: {pending_decision.get('content', 'ì•Œ ìˆ˜ ì—†ëŠ” ìš”ì²­')}
ìŠ¹ì¸í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (approved/rejected)
"""

    def _decide_after_analysis(self, state: WorkflowState) -> Literal["simple", "complex"]:
        """ë¶„ì„ ë‹¨ê³„ í›„ ë‹¨ìˆœ/ë³µì¡ ì¿¼ë¦¬ ê²°ì •"""
        current_step = state.get("current_step", "")

        if current_step == "simple_query":
            self.logger.info("ê°„ë‹¨í•œ ì¿¼ë¦¬ë¡œ íŒë‹¨ - ì§ì ‘ ë‹µë³€ ìƒì„±")
            return "simple"
        else:
            self.logger.info("ë³µì¡í•œ ì¿¼ë¦¬ë¡œ íŒë‹¨ - ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰")
            return "complex"

    def _decide_after_planning(self, state: WorkflowState) -> Literal[
        "execute", "need_approval", "skip_to_synthesize", "end"]:
        """ê³„íš ë‹¨ê³„ í›„ ë‹¤ìŒ ë™ì‘ ê²°ì • (HITL í¬í•¨)"""
        if state["iteration_count"] >= state["max_iterations"]:
            return "end"

        current_step = state.get("current_step", "")

        if current_step == "plan_skipped":
            return "skip_to_synthesize"
        elif current_step == "no_suitable_tools":
            return "skip_to_synthesize"
        elif current_step == "plan_ready":
            # ë„êµ¬ ì‹¤í–‰ ì „ ìŠ¹ì¸ì´ í•„ìš”í•œì§€ ì²´í¬
            if self._needs_approval_for_tools(state):
                return "need_approval"
            else:
                return "execute"
        elif current_step == "plan_failed":
            return "end"
        else:
            return "execute"

    def _needs_approval_for_tools(self, state: WorkflowState) -> bool:
        """ë„êµ¬ ì‹¤í–‰ ì „ ìŠ¹ì¸ì´ í•„ìš”í•œì§€ íŒë‹¨"""
        if not self.hitl_config.get("enabled", False):
            return False

        if self.hitl_config.get("require_approval_for_tools", False):
            # ê³ ìœ„í—˜ ë„êµ¬ì¸ì§€ ì²´í¬
            high_impact_tools = self.hitl_config.get("high_impact_tools", [])
            for tool in self.tools:
                tool_name = getattr(tool, 'name', str(tool))
                if any(risk_tool in tool_name.lower() for risk_tool in high_impact_tools):
                    state["pending_decision"] = {
                        "type": HumanApprovalType.TOOL_EXECUTION.value,
                        "tool_name": tool_name,
                        "tool_args": {"query": state["user_query"]},
                        "reason": "ê³ ìœ„í—˜ ë„êµ¬ ì‹¤í–‰"
                    }
                    return True

        return False

    def _decide_after_approval(self, state: WorkflowState) -> Literal["approved", "rejected", "modified", "need_input"]:
        """Human approval í›„ ê²°ì •"""
        human_response = state.get("human_response", "approved").lower()

        if "approved" in human_response or "ìŠ¹ì¸" in human_response:
            return "approved"
        elif "rejected" in human_response or "ê±°ë¶€" in human_response:
            return "rejected"
        elif "modified" in human_response or "ìˆ˜ì •" in human_response:
            return "modified"
        elif "input" in human_response or "ì…ë ¥" in human_response:
            return "need_input"
        else:
            return "approved"  # ê¸°ë³¸ê°’

    def _decide_next_step(self, state: WorkflowState) -> Literal["continue", "synthesize", "need_approval", "end"]:
        """ë‹¤ìŒ ë‹¨ê³„ ê²°ì • (HITL í¬í•¨)"""
        # ë¨¼ì € ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ì²´í¬
        if state["iteration_count"] >= state["max_iterations"]:
            self.logger.info(f"ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜({state['max_iterations']}) ë„ë‹¬ - ì¢…ë£Œ")
            return "end"

        if not state["evaluation_results"]:
            self.logger.info("í‰ê°€ ê²°ê³¼ê°€ ì—†ìŒ - ê³„ì† ì§„í–‰")
            return "continue"

        latest_evaluation = state["evaluation_results"][-1]
        evaluation_type = latest_evaluation.get("evaluation", "")
        confidence = latest_evaluation.get("confidence", 0.0)

        self.logger.info(
            f"í‰ê°€ ê²°ê³¼ í™•ì¸: type={evaluation_type}, confidence={confidence:.2f}, iteration={state['iteration_count']}")

        # ë‚®ì€ ì‹ ë¢°ë„ì— ëŒ€í•œ Human approval ì²´í¬
        if (self.hitl_config.get("enabled", False) and
                self.hitl_config.get("require_approval_for_low_confidence", False) and
                confidence < self.hitl_config.get("confidence_threshold", 0.7)):
            state["pending_decision"] = {
                "type": HumanApprovalType.LOW_CONFIDENCE.value,
                "confidence": confidence,
                "result": latest_evaluation.get("reason", ""),
                "evaluation_type": evaluation_type
            }
            return "need_approval"

        # ê¸°ì¡´ ë¡œì§
        if confidence >= 1.0:
            self.logger.info(f"ì™„ë²½í•œ ì‹ ë¢°ë„({confidence:.2f}) ë‹¬ì„± - ì¦‰ì‹œ ë‹µë³€ í•©ì„±")
            return "synthesize"
        elif confidence >= 0.95 and evaluation_type == ToolEvaluationResult.SUCCESS.value:
            self.logger.info(f"ë§¤ìš° ë†’ì€ ì‹ ë¢°ë„({confidence:.2f})ì™€ ì„±ê³µ ê²°ê³¼ - ë‹µë³€ í•©ì„±")
            return "synthesize"
        elif evaluation_type == ToolEvaluationResult.SUCCESS.value and confidence >= 0.8:
            self.logger.info(f"ë†’ì€ ì‹ ë¢°ë„({confidence:.2f})ì™€ ì„±ê³µ ê²°ê³¼ - ë‹µë³€ í•©ì„±")
            return "synthesize"
        elif evaluation_type == ToolEvaluationResult.NEEDS_MORE_INFO.value and confidence < 0.9:
            self.logger.info(f"ì¶”ê°€ ì •ë³´ í•„ìš”({confidence:.2f}) - ê³„ì† ì§„í–‰")
            return "continue"
        elif confidence >= 0.7:
            self.logger.info(f"ì ì ˆí•œ ì‹ ë¢°ë„({confidence:.2f}) - ë‹µë³€ í•©ì„±")
            return "synthesize"
        else:
            self.logger.info(f"ë‚®ì€ ì‹ ë¢°ë„({confidence:.2f}) - ê³„ì† ì§„í–‰")
            return "continue"

    def _decide_final_step(self, state: WorkflowState) -> Literal["approved", "retry", "need_approval"]:
        """ìµœì¢… ë‹¨ê³„ ê²°ì • (HITL í¬í•¨)"""
        if state["iteration_count"] >= state["max_iterations"]:
            return "approved"

        # ìµœì¢… ë‹µë³€ ìŠ¹ì¸ì´ í•„ìš”í•œì§€ ì²´í¬
        if (self.hitl_config.get("enabled", False) and
                self.hitl_config.get("require_approval_for_final_answer", False)):
            state["pending_decision"] = {
                "type": HumanApprovalType.FINAL_ANSWER.value,
                "answer": state.get("final_answer", "")
            }
            return "need_approval"

        return state.get("next_action", "approved")

    async def chat_stream(self, message: str, thread_id: str = "default", hitl_enabled: bool = None) -> AsyncGenerator[
        str, None]:
        """Human-in-the-loop ì§€ì› ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ…"""
        if not self.workflow:
            yield "ì›Œí¬í”Œë¡œìš°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            return

        # HITL ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ
        if hitl_enabled is not None:
            original_hitl = self.hitl_config["enabled"]
            self.hitl_config["enabled"] = hitl_enabled

        # ì´ˆê¸° ìƒíƒœ ì„¤ì •
        initial_state = WorkflowState(
            messages=[HumanMessage(content=message)],
            user_query=message,
            current_step="start",
            tool_results=[],
            evaluation_results=[],
            confidence_score=0.0,
            next_action="",
            iteration_count=0,
            max_iterations=3,
            final_answer="",
            reasoning_trace=[],
            # HITL í•„ë“œ ì´ˆê¸°í™”
            human_approval_needed=False,
            human_input_requested=False,
            human_response=None,
            pending_decision=None,
            hitl_enabled=self.hitl_config.get("enabled", False)
        )

        config = {
            "configurable": {"thread_id": thread_id},
            "recursion_limit": 50
        }

        try:
            final_state = None
            step_count = 0
            max_steps = 15  # ìŠ¤í… ìˆ˜ë¥¼ ë” ì¤„ì„ (25 -> 15)
            workflow_completed = False
            answer_provided = False

            async for event in self.workflow.astream(initial_state, config=config):
                step_count += 1
                if step_count > max_steps:
                    self.logger.error(f"ìµœëŒ€ ìŠ¤í… ìˆ˜({max_steps}) ì´ˆê³¼ - ê°•ì œ ì¢…ë£Œ")
                    break

                for node_name, node_state in event.items():
                    # Human approvalì´ë‚˜ inputì´ í•„ìš”í•œ ê²½ìš° ì‚¬ìš©ìì—ê²Œ ì•Œë¦¼
                    if node_name == "human_approval":
                        pending_decision = node_state.get("pending_decision", {})
                        approval_message = self._create_approval_message(node_state,
                                                                         pending_decision.get("type", ""),
                                                                         pending_decision)
                        yield f"\nğŸ¤š **Human Approval í•„ìš”**\n{approval_message}\n"

                    elif node_name == "human_input":
                        yield f"\nğŸ’­ **Human Input í•„ìš”**\nì¶”ê°€ ì •ë³´ë‚˜ ì§€ì‹œì‚¬í•­ì„ ì œê³µí•´ì£¼ì„¸ìš”.\n"

                    # ì‹œìŠ¤í…œ ë¡œê·¸
                    if node_state.get("current_step"):
                        self.logger.info(f"ì›Œí¬í”Œë¡œìš° ë‹¨ê³„: {node_state['current_step']} (ë…¸ë“œ: {node_name}) - ìŠ¤í…: {step_count}")

                    final_state = node_state

                    # ë‹µë³€ì´ ì´ë¯¸ ì œê³µë˜ì—ˆëŠ”ì§€ ì²´í¬
                    if node_state.get("final_answer") and not answer_provided:
                        # ì²« ë²ˆì§¸ final_answerê°€ ìƒì„±ë˜ë©´ ì‚¬ìš©ìì—ê²Œ ì œê³µ
                        if node_name in ["synthesize_answer", "simple_answer"]:
                            confidence = node_state.get("confidence_score", 0.0)
                            iterations = node_state.get("iteration_count", 0)
                            self.logger.info(f"ìµœì¢… ë‹µë³€ ìƒì„± - ì‹ ë¢°ë„: {confidence:.2f}, ë°˜ë³µ: {iterations}, ìŠ¤í…: {step_count}")

                            yield node_state["final_answer"]
                            answer_provided = True

                            # quality_checkê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆê±°ë‚˜ ê°„ë‹¨í•œ ë‹µë³€ì´ë©´ ì¦‰ì‹œ ì¢…ë£Œ
                            if (node_name == "simple_answer" or
                                    not self.hitl_config.get("require_approval_for_final_answer", False)):
                                self.logger.info("ë‹µë³€ ì œê³µ ì™„ë£Œ - ì›Œí¬í”Œë¡œìš° ì¡°ê¸° ì¢…ë£Œ")
                                workflow_completed = True
                                break

                    # quality_checkì—ì„œ approvedê°€ ë‚˜ì˜¤ë©´ ì¢…ë£Œ
                    elif (node_name == "quality_check" and
                          node_state.get("next_action") == "approved" and
                          answer_provided):
                        self.logger.info("í’ˆì§ˆ ê²€ì‚¬ í†µê³¼ - ì›Œí¬í”Œë¡œìš° ì¢…ë£Œ")
                        workflow_completed = True
                        break

                # ì›Œí¬í”Œë¡œìš°ê°€ ì™„ë£Œë˜ë©´ ì™¸ë¶€ ë£¨í”„ë„ ì¢…ë£Œ
                if workflow_completed:
                    break

            # ë‹µë³€ì´ ì œê³µë˜ì§€ ì•Šì•˜ë‹¤ë©´ ë§ˆì§€ë§‰ìœ¼ë¡œ ì‹œë„
            if not answer_provided and final_state and final_state.get("final_answer"):
                self.logger.info("ë§ˆì§€ë§‰ ì‹œë„ë¡œ ë‹µë³€ ì œê³µ")
                yield final_state["final_answer"]
            elif not answer_provided:
                yield "ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

        except Exception as e:
            self.logger.error(f"HITL ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            yield f"ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

        finally:
            # HITL ì„¤ì • ë³µì›
            if hitl_enabled is not None:
                self.hitl_config["enabled"] = original_hitl

    def create_model(self, model_name: str):
        """ëª¨ë¸ ìƒì„±"""
        output_tokens = {
            "claude-3-5-sonnet-latest": 8192,
            "claude-3-5-haiku-latest": 8192,
            "claude-3-7-sonnet-latest": 64000,
            "gpt-4o": 16000,
            "gpt-4o-mini": 16000,
        }

        if model_name.startswith("claude"):
            return ChatAnthropic(
                model_name=model_name,
                temperature=0.1
            )
        elif model_name.startswith("gpt"):
            return ChatOpenAI(
                model=model_name,
                temperature=0.1,
                max_tokens=output_tokens.get(model_name, 16000)
            )
        elif model_name.startswith("qwen2.5:32b"):
            return ChatOllama(
                model="qwen2.5:32b",
                temperature=0.1
            )
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model_name}")

    def load_mcp_config(self) -> Dict:
        """MCP ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        config_path = "mcp-config/mcp_config.json"
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            self.logger.warning(f"MCP ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}")
            return {"mcpServers": {}}
        except Exception as e:
            self.logger.error(f"MCP ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {"mcpServers": {}}

    async def get_agent_status(self) -> Dict[str, Any]:
        """ì—ì´ì „íŠ¸ ìƒíƒœ ì •ë³´ (HITL ì •ë³´ í¬í•¨)"""
        tools_count = len(self.tools) if self.tools else 0

        return {
            "is_initialized": self.workflow is not None,
            "model_name": getattr(self.model, 'model_name', 'Unknown') if self.model else None,
            "evaluator_model": getattr(self.evaluator_model, 'model_name', 'Unknown') if self.evaluator_model else None,
            "tools_count": tools_count,
            "mcp_client_active": self.mcp_client is not None,
            "workflow_active": self.workflow is not None,
            # HITL ìƒíƒœ ì •ë³´
            "hitl_config": self.hitl_config,
            "human_input_callback_set": self.human_input_callback is not None
        }

    async def cleanup_mcp_client(self):
        """MCP í´ë¼ì´ì–¸íŠ¸ ì •ë¦¬"""
        if self.mcp_client:
            try:
                await self.mcp_client.__aexit__(None, None, None)
            except Exception as e:
                self.logger.warning(f"MCP í´ë¼ì´ì–¸íŠ¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            finally:
                self.mcp_client = None