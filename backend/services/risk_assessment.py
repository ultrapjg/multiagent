import json
import logging
from enum import Enum
from typing import Dict, List, Any, Optional, Tuple
from langchain_core.prompts import ChatPromptTemplate


class RiskLevel(Enum):
    """ìœ„í—˜ë„ ë ˆë²¨"""
    SAFE = "safe"
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


class RiskCategory(Enum):
    """ìœ„í—˜ ì¹´í…Œê³ ë¦¬"""
    DATA_DESTRUCTION = "data_destruction"
    SYSTEM_MODIFICATION = "system_modification"
    SECURITY_BREACH = "security_breach"
    FINANCIAL_IMPACT = "financial_impact"
    PRIVACY_VIOLATION = "privacy_violation"
    SERVICE_DISRUPTION = "service_disruption"
    IRREVERSIBLE_ACTION = "irreversible_action"


class LLMRiskAssessmentService:
    """LLM ê¸°ë°˜ ìœ„í—˜ë„ í‰ê°€ ì„œë¹„ìŠ¤"""

    def __init__(self, model, evaluator_model=None):
        self.model = model
        self.evaluator_model = evaluator_model or model
        self.logger = logging.getLogger(__name__)

        # ìœ„í—˜ë„ ì„ê³„ê°’ ì„¤ì •
        self.risk_thresholds = {
            RiskLevel.SAFE: 0.0,
            RiskLevel.LOW: 0.2,
            RiskLevel.MEDIUM: 0.4,
            RiskLevel.HIGH: 0.6,
            RiskLevel.CRITICAL: 0.8
        }

    async def assess_risk(self,
                          user_query: str,
                          planned_tools: List[str],
                          tool_descriptions: Dict[str, str] = None,
                          context: Dict[str, Any] = None) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì¿¼ë¦¬ì™€ ê³„íšëœ ë„êµ¬ë“¤ì˜ ìœ„í—˜ë„ë¥¼ LLMìœ¼ë¡œ í‰ê°€"""

        self.logger.info(f"LLM ìœ„í—˜ë„ í‰ê°€ ì‹œì‘: {user_query}")

        # ë„êµ¬ ì„¤ëª… ì •ë³´ ì¤€ë¹„
        tools_info = self._prepare_tools_info(planned_tools, tool_descriptions)

        # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì¤€ë¹„
        context_info = self._prepare_context_info(context or {})

        # ìœ„í—˜ë„ í‰ê°€ í”„ë¡¬í”„íŠ¸
        risk_assessment_prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ì‚¬ì´ë²„ ë³´ì•ˆ ë° ì‹œìŠ¤í…œ ì•ˆì „ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ ìš”ì²­ê³¼ ì‹¤í–‰ ì˜ˆì •ì¸ ë„êµ¬ë“¤ì„ ë¶„ì„í•˜ì—¬ ìœ„í—˜ë„ë¥¼ í‰ê°€í•´ì£¼ì„¸ìš”.

**í‰ê°€ ê¸°ì¤€:**

ğŸ”´ **CRITICAL (0.8-1.0)**: ì¦‰ê°ì ì´ê³  ëŒì´í‚¬ ìˆ˜ ì—†ëŠ” ì‹¬ê°í•œ í”¼í•´ ê°€ëŠ¥
- ì‹œìŠ¤í…œ ì „ì²´ íŒŒê´´, ì¤‘ìš” ë°ì´í„° ì™„ì „ ì‚­ì œ
- ë³´ì•ˆ ì‹œìŠ¤í…œ ì™„ì „ ë¬´ë ¥í™”
- ì¬ì •ì ìœ¼ë¡œ ì‹¬ê°í•œ ì†ì‹¤ (ìˆ˜ë°±ë§Œì› ì´ìƒ)

ğŸŸ  **HIGH (0.6-0.8)**: ìƒë‹¹í•œ í”¼í•´ë‚˜ ë³µêµ¬ ë¹„ìš© ë°œìƒ ê°€ëŠ¥
- ì¤‘ìš” íŒŒì¼/í´ë” ì‚­ì œ, ì‹œìŠ¤í…œ ì„¤ì • ë³€ê²½
- ë¯¼ê°í•œ ì •ë³´ ë…¸ì¶œ ìœ„í—˜
- ì„œë¹„ìŠ¤ ì¤‘ë‹¨ ê°€ëŠ¥ì„±

ğŸŸ¡ **MEDIUM (0.4-0.6)**: ì œí•œì  í”¼í•´ë‚˜ ë³µêµ¬ ê°€ëŠ¥í•œ ë¬¸ì œ
- ì¼ë°˜ íŒŒì¼ ìˆ˜ì •/ì‚­ì œ
- ì„¤ì • ë³€ê²½ (ë³µêµ¬ ê°€ëŠ¥)
- ì„ì‹œì  ì„œë¹„ìŠ¤ ì˜í–¥

ğŸŸ¢ **LOW (0.2-0.4)**: ê²½ë¯¸í•œ ì˜í–¥, ì‰½ê²Œ ë³µêµ¬ ê°€ëŠ¥
- ë¡œê·¸ íŒŒì¼ ì¡°íšŒ, ì„ì‹œ íŒŒì¼ ìƒì„±
- ì½ê¸° ì „ìš© ì‘ì—…

âšª **SAFE (0.0-0.2)**: ìœ„í—˜ ì—†ìŒ
- ì •ë³´ ì¡°íšŒ, ì‹œê°„ í™•ì¸, ê³„ì‚°

**íŠ¹ë³„ ê³ ë ¤ì‚¬í•­:**
- ì‚¬ìš©ìì˜ ì˜ë„ì™€ ë§¥ë½ì„ ê³ ë ¤í•˜ì„¸ìš”
- ë„êµ¬ì˜ ì‹¤ì œ ê¸°ëŠ¥ê³¼ ê¶Œí•œì„ ë¶„ì„í•˜ì„¸ìš”  
- ë³µêµ¬ ê°€ëŠ¥ì„±ê³¼ ì˜í–¥ ë²”ìœ„ë¥¼ í‰ê°€í•˜ì„¸ìš”
- ì—°ì‡„ ë°˜ì‘ì´ë‚˜ ë¶€ì‘ìš©ì„ ê³ ë ¤í•˜ì„¸ìš”

**í˜„ì¬ ìƒí™©:**
ì‚¬ìš©ì ìš”ì²­: {user_query}
ì‹¤í–‰ ì˜ˆì • ë„êµ¬ë“¤: {tools_info}
ì»¨í…ìŠ¤íŠ¸: {context_info}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ í‰ê°€ ê²°ê³¼ë¥¼ ì œê³µí•˜ì„¸ìš”:
{{
  "overall_risk_score": 0.0-1.0,
  "risk_level": "safe|low|medium|high|critical",
  "risk_categories": ["category1", "category2"],
  "risk_analysis": {{
    "potential_damages": ["í”¼í•´ ëª©ë¡"],
    "affected_resources": ["ì˜í–¥ë°›ì„ ìì›ë“¤"],
    "reversibility": "ì‰½ê²Œë³µêµ¬ê°€ëŠ¥|ì–´ë ¤ì›€|ë¶ˆê°€ëŠ¥",
    "impact_scope": "ê°œì¸|íŒ€|ì¡°ì§|ì „ì²´ì‹œìŠ¤í…œ"
  }},
  "approval_required": true/false,
  "approval_reason": "ìŠ¹ì¸ì´ í•„ìš”í•œ êµ¬ì²´ì  ì´ìœ ",
  "mitigation_suggestions": ["ìœ„í—˜ ì™„í™” ë°©ì•ˆë“¤"],
  "alternative_approaches": ["ë” ì•ˆì „í•œ ëŒ€ì•ˆë“¤"],
  "confidence": 0.0-1.0
}}"""),
            ("human", "ìœ„ ìš”ì²­ì˜ ìœ„í—˜ë„ë¥¼ í‰ê°€í•´ì£¼ì„¸ìš”.")
        ])

        try:
            # LLM ìœ„í—˜ë„ í‰ê°€ ì‹¤í–‰
            response = await self.model.ainvoke(
                risk_assessment_prompt.format_messages(
                    user_query=user_query,
                    tools_info=tools_info,
                    context_info=context_info
                )
            )

            # JSON íŒŒì‹± ë° ê²€ì¦
            risk_assessment = await self._parse_and_validate_assessment(response.content)

            # ìœ„í—˜ë„ ì„ê³„ê°’ ê¸°ë°˜ ìŠ¹ì¸ í•„ìš” ì—¬ë¶€ ê²°ì •
            needs_approval = self._determine_approval_requirement(risk_assessment)
            risk_assessment["approval_required"] = needs_approval

            self.logger.info(
                f"ìœ„í—˜ë„ í‰ê°€ ì™„ë£Œ: {risk_assessment['risk_level']} ({risk_assessment['overall_risk_score']:.2f})")

            return risk_assessment

        except Exception as e:
            self.logger.error(f"LLM ìœ„í—˜ë„ í‰ê°€ ì‹¤íŒ¨: {e}")
            # ì•ˆì „ì„ ìœ„í•œ í´ë°±: ì•Œ ìˆ˜ ì—†ëŠ” ê²½ìš° ê³ ìœ„í—˜ìœ¼ë¡œ ë¶„ë¥˜
            return self._create_fallback_assessment(user_query, planned_tools, str(e))

    async def generate_approval_message(self,
                                        risk_assessment: Dict[str, Any],
                                        user_query: str) -> str:
        """ìœ„í—˜ë„ í‰ê°€ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìŠ¹ì¸ ë©”ì‹œì§€ ìƒì„±"""

        risk_level = risk_assessment.get("risk_level", "high")
        risk_score = risk_assessment.get("overall_risk_score", 0.8)
        potential_damages = risk_assessment.get("risk_analysis", {}).get("potential_damages", [])
        mitigation_suggestions = risk_assessment.get("mitigation_suggestions", [])

        # ìœ„í—˜ë„ë³„ ì´ëª¨ì§€ ë° ìƒ‰ìƒ
        risk_indicators = {
            "critical": "ğŸš¨",
            "high": "ğŸ”´",
            "medium": "ğŸŸ¡",
            "low": "ğŸŸ¢",
            "safe": "âšª"
        }

        indicator = risk_indicators.get(risk_level, "âš ï¸")

        message = f"""{indicator} **ìœ„í—˜ë„ í‰ê°€ ê²°ê³¼**

**ìš”ì²­ ë‚´ìš©:** {user_query}
**ìœ„í—˜ë„:** {risk_level.upper()} ({risk_score:.1f}/1.0)
**í‰ê°€ ì´ìœ :** {risk_assessment.get('approval_reason', 'ìœ„í—˜ ìš”ì†Œ ê°ì§€')}

"""

        if potential_damages:
            message += f"**ì˜ˆìƒ í”¼í•´:**\n"
            for damage in potential_damages[:3]:  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
                message += f"â€¢ {damage}\n"
            message += "\n"

        if mitigation_suggestions:
            message += f"**ìœ„í—˜ ì™„í™” ë°©ì•ˆ:**\n"
            for suggestion in mitigation_suggestions[:2]:  # ìƒìœ„ 2ê°œë§Œ í‘œì‹œ
                message += f"â€¢ {suggestion}\n"
            message += "\n"

        message += """**ìŠ¹ì¸ ì˜µì…˜:**
â€¢ `approved` - ìœ„í—˜ì„ ê°ìˆ˜í•˜ê³  ì§„í–‰
â€¢ `rejected` - ì‘ì—… ì¤‘ë‹¨
â€¢ `modified` - ë” ì•ˆì „í•œ ë°©ë²• ì œì•ˆ ìš”ì²­

ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"""

        return message

    def _prepare_tools_info(self, planned_tools: List[str], tool_descriptions: Dict[str, str] = None) -> str:
        """ë„êµ¬ ì •ë³´ ì¤€ë¹„"""
        if not planned_tools:
            return "ì‹¤í–‰ ì˜ˆì • ë„êµ¬ ì—†ìŒ"

        tools_info = []
        for tool_name in planned_tools:
            if tool_descriptions and tool_name in tool_descriptions:
                tools_info.append(f"- {tool_name}: {tool_descriptions[tool_name]}")
            else:
                tools_info.append(f"- {tool_name}")

        return "\n".join(tools_info)

    def _prepare_context_info(self, context: Dict[str, Any]) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì¤€ë¹„"""
        context_parts = []

        if context.get("user_role"):
            context_parts.append(f"ì‚¬ìš©ì ì—­í• : {context['user_role']}")

        if context.get("system_state"):
            context_parts.append(f"ì‹œìŠ¤í…œ ìƒíƒœ: {context['system_state']}")

        if context.get("previous_actions"):
            context_parts.append(f"ì´ì „ ì‘ì—…: {', '.join(context['previous_actions'])}")

        if context.get("sensitive_data_present"):
            context_parts.append("ë¯¼ê°í•œ ë°ì´í„° ì¡´ì¬: ì˜ˆ")

        return " | ".join(context_parts) if context_parts else "ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ"

    async def _parse_and_validate_assessment(self, response_content: str) -> Dict[str, Any]:
        """LLM ì‘ë‹µ íŒŒì‹± ë° ê²€ì¦"""
        try:
            assessment = json.loads(response_content)

            # í•„ìˆ˜ í•„ë“œ ê²€ì¦
            required_fields = ["overall_risk_score", "risk_level", "approval_reason"]
            for field in required_fields:
                if field not in assessment:
                    raise ValueError(f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}")

            # ìœ„í—˜ë„ ì ìˆ˜ ê²€ì¦
            risk_score = assessment["overall_risk_score"]
            if not (0.0 <= risk_score <= 1.0):
                raise ValueError(f"ì˜ëª»ëœ ìœ„í—˜ë„ ì ìˆ˜: {risk_score}")

            # ì‹ ë¢°ë„ ê²€ì¦
            confidence = assessment.get("confidence", 0.5)
            if not (0.0 <= confidence <= 1.0):
                assessment["confidence"] = 0.5

            return assessment

        except json.JSONDecodeError as e:
            self.logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            raise ValueError(f"LLM ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜: {e}")
        except Exception as e:
            self.logger.error(f"í‰ê°€ ê²°ê³¼ ê²€ì¦ ì‹¤íŒ¨: {e}")
            raise

    def _determine_approval_requirement(self, risk_assessment: Dict[str, Any]) -> bool:
        """ìœ„í—˜ë„ í‰ê°€ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìŠ¹ì¸ í•„ìš” ì—¬ë¶€ ê²°ì •"""
        risk_score = risk_assessment.get("overall_risk_score", 0.8)
        confidence = risk_assessment.get("confidence", 0.5)

        # ê³ ìœ„í—˜ (0.6 ì´ìƒ) ë˜ëŠ” ì‹ ë¢°ë„ê°€ ë‚®ì€ ê²½ìš° (0.7 ë¯¸ë§Œ) ìŠ¹ì¸ í•„ìš”
        if risk_score >= 0.6:
            return True

        if confidence < 0.7 and risk_score >= 0.4:
            return True

        # íŠ¹ì • ìœ„í—˜ ì¹´í…Œê³ ë¦¬ê°€ ìˆëŠ” ê²½ìš°
        risk_categories = risk_assessment.get("risk_categories", [])
        critical_categories = [
            RiskCategory.DATA_DESTRUCTION.value,
            RiskCategory.SECURITY_BREACH.value,
            RiskCategory.IRREVERSIBLE_ACTION.value
        ]

        if any(cat in critical_categories for cat in risk_categories):
            return True

        return False

    def _create_fallback_assessment(self, user_query: str, planned_tools: List[str], error: str) -> Dict[str, Any]:
        """í‰ê°€ ì‹¤íŒ¨ ì‹œ ì•ˆì „í•œ í´ë°± í‰ê°€"""
        self.logger.warning(f"í´ë°± ìœ„í—˜ë„ í‰ê°€ ì‚¬ìš©: {error}")

        return {
            "overall_risk_score": 0.8,  # ì•ˆì „ì„ ìœ„í•´ ë†’ì€ ìœ„í—˜ë„
            "risk_level": RiskLevel.HIGH.value,
            "risk_categories": [RiskCategory.SYSTEM_MODIFICATION.value],
            "risk_analysis": {
                "potential_damages": ["ì•Œ ìˆ˜ ì—†ëŠ” ìœ„í—˜ìœ¼ë¡œ ì¸í•œ ì ì¬ì  í”¼í•´"],
                "affected_resources": ["ì‹œìŠ¤í…œ ì „ë°˜"],
                "reversibility": "ë¶ˆí™•ì‹¤",
                "impact_scope": "ì•Œ ìˆ˜ ì—†ìŒ"
            },
            "approval_required": True,
            "approval_reason": f"ìœ„í—˜ë„ í‰ê°€ ì‹¤íŒ¨ë¡œ ì¸í•œ ì•ˆì „ ì¡°ì¹˜ (ì˜¤ë¥˜: {error})",
            "mitigation_suggestions": [
                "ì‘ì—…ì„ ë” ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„í• ",
                "ë°±ì—… ìƒì„± í›„ ì§„í–‰",
                "í…ŒìŠ¤íŠ¸ í™˜ê²½ì—ì„œ ë¨¼ì € ì‹œë„"
            ],
            "alternative_approaches": ["ìˆ˜ë™ í™•ì¸ í›„ ì§„í–‰"],
            "confidence": 0.3,
            "fallback_used": True,
            "original_error": error
        }